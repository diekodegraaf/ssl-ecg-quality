{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce251729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "def set_seed(seed: int = 42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print('Setting seed to', seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b48b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "import scipy\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Helper function to calculate entropy for a batch of signals\n",
    "def calculate_entropy(batch_values):\n",
    "    \"\"\"\n",
    "    Calculate entropy for a batch of signals.\n",
    "    Each row in `batch_values` is a separate signal.\n",
    "    \"\"\"\n",
    "    entropies = []\n",
    "    for list_values in batch_values:\n",
    "        counter_values = Counter(list_values.flatten()).most_common()\n",
    "        probabilities = [elem[1] / len(list_values) for elem in counter_values]\n",
    "        entropy = scipy.stats.entropy(probabilities)\n",
    "        entropies.append(entropy)\n",
    "    return np.array(entropies)[:, np.newaxis]\n",
    "\n",
    "# Helper function to calculate statistics for a batch of signals\n",
    "def calculate_statistics(batch_values):\n",
    "    \"\"\"\n",
    "    Calculate statistics for a batch of signals.\n",
    "    Each row in `batch_values` is a separate signal.\n",
    "    \"\"\"\n",
    "    # n5 = np.nanpercentile(batch_values, 5, axis=1)\n",
    "    # n25 = np.nanpercentile(batch_values, 25, axis=1)\n",
    "    # n75 = np.nanpercentile(batch_values, 75, axis=1)\n",
    "    # n95 = np.nanpercentile(batch_values, 95, axis=1)\n",
    "    medians = np.nanpercentile(batch_values, 50, axis=1)\n",
    "    means = np.nanmean(batch_values, axis=1)\n",
    "    stds = np.nanstd(batch_values, axis=1)\n",
    "    vars = np.nanvar(batch_values, axis=1)\n",
    "    rms = np.nanmean(np.sqrt(batch_values**2), axis=1)\n",
    "    return np.column_stack([medians, means, stds, vars, rms])\n",
    "\n",
    "# Helper function to calculate crossings for a batch of signals\n",
    "def calculate_crossings(batch_values):\n",
    "    \"\"\"\n",
    "    Calculate zero and mean crossings for a batch of signals.\n",
    "    `batch_values` is (batch_size x wavdec coefficients).\n",
    "    \"\"\"\n",
    "    # Zero crossings: Check where the sign changes\n",
    "    zero_crossings = np.sum(np.diff(np.sign(batch_values), axis=1) != 0, axis=1)\n",
    "\n",
    "    # Mean crossings: Subtract the mean and check sign changes\n",
    "    mean_values = np.nanmean(batch_values, axis=1, keepdims=True)\n",
    "    mean_crossings = np.sum(np.diff(np.sign(batch_values - mean_values), axis=1) != 0, axis=1)\n",
    "    \n",
    "    return zero_crossings, mean_crossings\n",
    "\n",
    "# Function to extract features for a batch of signals\n",
    "def get_features(batch_values):\n",
    "    \"\"\"\n",
    "    Extract features (entropy, crossings, and statistics) for a batch of signals.\n",
    "    Each row in `batch_values` is a separate signal.\n",
    "    \"\"\"\n",
    "    # entropy value is the same for each signals coefficients\n",
    "    entropies = calculate_entropy(batch_values)\n",
    "    # print(entropies.shape)\n",
    "    zero_crossings, mean_crossings = calculate_crossings(batch_values)\n",
    "    # print(zero_crossings.shape, mean_crossings.shape)\n",
    "    statistics = calculate_statistics(batch_values)\n",
    "    # print(statistics.shape)\n",
    "    # Combine all features in a batch-wise manner\n",
    "    return np.column_stack([entropies, zero_crossings, mean_crossings, statistics])\n",
    "\n",
    "# Feature Extraction Function (for batched signals)\n",
    "def batched_feature_extraction(batch_signals, wavelet='sym5'):\n",
    "    \"\"\"\n",
    "    Extract wavelet features for a batch of signals.\n",
    "    Each row in `batch_signals` is a separate signal. (n_samples, window_length)\n",
    "    Out: feature matrix (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    # Perform wavelet decomposition on batch and get dict,\n",
    "    # with 'a' (approximation, low-freq) and 'd' (detail, high-freq) coefficients for each level\n",
    "    coeffs_batch = pywt.wavedecn(batch_signals, wavelet, mode='symmetric', level=4, axes=[1])\n",
    "    # NOTE: possibly padd the signals symmetricly for level 5\n",
    "    # print(len(coeffs_batch), coeffs_batch[0].shape, coeffs_batch[1]['d'].shape)\n",
    "    # batch_size 32 x 24 components for level 4, 40 for level 5\n",
    "    # I want 32 x 24 x 8\n",
    "  \n",
    "    for item in coeffs_batch:\n",
    "        if isinstance(item, dict):\n",
    "            signal_coeffs = list(item.values())[0]\n",
    "        else:\n",
    "            signal_coeffs = item\n",
    "            \n",
    "        # print('signal_coeffs', signal_coeffs.shape)\n",
    "        features.append(get_features(signal_coeffs))\n",
    "    stacked_features = np.concatenate(features, axis=1)\n",
    "    # print(stacked_features.shape)\n",
    "    return stacked_features\n",
    "\n",
    "# Prepare annotated data for batched signals during validation and test\n",
    "def prepare_ann_data(loader):\n",
    "    \"\"\"\n",
    "    Prepare features and labels for a batched signal loader.\n",
    "    Input `loader` should provide batches as 2D arrays (num_signals x window_length_samples).\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (signals, batch_labels) in enumerate(tqdm.tqdm(loader)):\n",
    "        batch_features = batched_feature_extraction(signals)\n",
    "        all_features.append(batch_features)\n",
    "        all_labels.append(batch_labels.numpy())\n",
    "        # Debug: break validation after x batches\n",
    "        if i == 100:\n",
    "            pass\n",
    "    feature_stack = np.vstack(all_features)\n",
    "    label_array = np.hstack(all_labels)\n",
    "    \n",
    "    # print(feature_stack.shape)\n",
    "    # print(label_array.shape)\n",
    "    return feature_stack, label_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16fb16",
   "metadata": {},
   "source": [
    "# Supervised KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def train_knn_classifier(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    model_filename=None,\n",
    "    batch_size=32,\n",
    "    n_neighbors=5,\n",
    "    load_model=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train or load a KNN classifier on labeled data.\n",
    "\n",
    "    Parameters:\n",
    "        train_dataset (Dataset): Annotated training dataset.\n",
    "        val_dataset (Dataset): Annotated validation dataset.\n",
    "        test_dataset (Dataset): Annotated test dataset.\n",
    "        model_filename (str): Path to save or load the model.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        load_model (bool): Whether to load a saved model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels for the validation dataset.\n",
    "    \"\"\"\n",
    "    if load_model and model_filename and os.path.exists(model_filename):\n",
    "        print(f\"Loading model from {model_filename}\")\n",
    "        knn = joblib.load(model_filename)\n",
    "    else:\n",
    "        print(\"Extracting training features...\")\n",
    "        X_train, Y_train = prepare_ann_data(DataLoader(train_dataset, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train, Y_train)\n",
    "\n",
    "        if not model_filename:\n",
    "            current_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "            model_filename = f\"knn_model_k{n_neighbors}_{current_time}.pkl\"\n",
    "        joblib.dump(knn, model_filename)\n",
    "        print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    # Validation phase\n",
    "    print(\"Validation...\")\n",
    "    X_val, Y_val = prepare_ann_data(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))\n",
    "    val_predictions = knn.predict(X_val)\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_val, val_predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(Y_val, val_predictions, digits=4))\n",
    "\n",
    "    # Test phase\n",
    "    print(\"Testing...\")\n",
    "    X_test, Y_test = prepare_ann_data(DataLoader(test_dataset, batch_size=batch_size, shuffle=False))\n",
    "    test_predictions = knn.predict(X_test)\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, test_predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(Y_test, test_predictions, digits=4))\n",
    "\n",
    "    return val_predictions, Y_val, test_predictions, Y_test, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0127f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def train_knn_classifier(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    model_filename=None,\n",
    "    batch_size=32,\n",
    "    n_neighbors=5,\n",
    "    load_model=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train or load a KNN classifier on labeled data.\n",
    "\n",
    "    Parameters:\n",
    "        train_dataset (Dataset): Annotated training dataset.\n",
    "        val_dataset (Dataset): Annotated validation dataset.\n",
    "        test_dataset (Dataset): Annotated test dataset.\n",
    "        model_filename (str): Path to save or load the model.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        load_model (bool): Whether to load a saved model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels for the validation dataset.\n",
    "    \"\"\"\n",
    "    if load_model and model_filename and os.path.exists(model_filename):\n",
    "        print(f\"Loading model from {model_filename}\")\n",
    "        knn = joblib.load(model_filename)\n",
    "    else:\n",
    "        print(\"Extracting training features...\")\n",
    "        global X_train, Y_train\n",
    "        if 'X_train' in globals() and 'Y_train' in globals():\n",
    "            print(\"Reusing cached training data\")\n",
    "        else:\n",
    "            X_train, Y_train = prepare_ann_data(DataLoader(train_dataset, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train, Y_train)\n",
    "\n",
    "        if not model_filename:\n",
    "            current_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "            model_filename = f\"knn_model_k{n_neighbors}_{current_time}.pkl\"\n",
    "        joblib.dump(knn, model_filename)\n",
    "        print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    # Validation phase\n",
    "    print(\"Validation...\")\n",
    "    global X_val, Y_val\n",
    "    if 'X_val' in globals() and 'Y_val' in globals():\n",
    "        print(\"Reusing cached validation data\")\n",
    "    else:\n",
    "        X_val, Y_val = prepare_ann_data(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))\n",
    "    val_predictions = knn.predict(X_val)\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_val, val_predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(Y_val, val_predictions, digits=4))\n",
    "\n",
    "    # Test phase\n",
    "    print(\"Testing...\")\n",
    "    global X_test, Y_test\n",
    "    if 'X_test' in globals() and 'Y_test' in globals():\n",
    "        print(\"Reusing cached test data\")\n",
    "    else:\n",
    "        X_test, Y_test = prepare_ann_data(DataLoader(test_dataset, batch_size=batch_size, shuffle=False))\n",
    "    test_predictions = knn.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, test_predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(Y_test, test_predictions, digits=4))\n",
    "\n",
    "    return val_predictions, Y_val, test_predictions, Y_test, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5b45e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 60434\n",
      "Train dataset size (no labels): 269346\n",
      "Validation dataset size: 39760\n",
      "Test dataset size: 39380\n"
     ]
    }
   ],
   "source": [
    "from butqdb_dataloaders import AnnotatedDataset, TrainDataset\n",
    "import yaml\n",
    "\n",
    "model_filename = \"finished_models/baselines/knn_model.pkl\"\n",
    "\n",
    "config_file = \"bolts_config.yaml\"\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "data_path = config[\"dataset\"][\"data_path\"]\n",
    "train_records = config[\"dataset\"][\"train_records\"]\n",
    "validation_records = config[\"dataset\"][\"val_records\"]\n",
    "test_records = config[\"dataset\"][\"test_records\"]\n",
    "sampling_frequency = config[\"dataset\"][\"signal_fs\"]\n",
    "window_size = 2.5\n",
    "stride = None\n",
    "\n",
    "window_size = int(window_size * sampling_frequency)\n",
    "\n",
    "train_dataset = AnnotatedDataset(data_path, train_records, window_size, sampling_frequency, stride=stride)\n",
    "train_dataset_nolabels= TrainDataset(data_path, train_records, window_size, sampling_frequency, stride=stride)\n",
    "val_dataset = AnnotatedDataset(data_path, validation_records, window_size, sampling_frequency, stride=stride)\n",
    "test_dataset = AnnotatedDataset(data_path, test_records, window_size, sampling_frequency, stride=stride)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Train dataset size (no labels):\", len(train_dataset_nolabels))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1816a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [02:04<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:20<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[15231  3423     7]\n",
      " [11650  7940    16]\n",
      " [  551   936     6]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5552    0.8162    0.6609     18661\n",
      "           2     0.6456    0.4050    0.4977     19606\n",
      "           3     0.2069    0.0040    0.0079      1493\n",
      "\n",
      "    accuracy                         0.5829     39760\n",
      "   macro avg     0.4692    0.4084    0.3888     39760\n",
      "weighted avg     0.5867    0.5829    0.5559     39760\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:19<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[22354  3646     0]\n",
      " [ 9038  3212     0]\n",
      " [   35   302   793]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7113    0.8598    0.7785     26000\n",
      "           2     0.4486    0.2622    0.3310     12250\n",
      "           3     1.0000    0.7018    0.8248      1130\n",
      "\n",
      "    accuracy                         0.6693     39380\n",
      "   macro avg     0.7200    0.6079    0.6447     39380\n",
      "weighted avg     0.6379    0.6693    0.6406     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=5,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e0a0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [02:02<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:19<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[16182  2472     7]\n",
      " [12986  6605    15]\n",
      " [  665   819     9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5424    0.8672    0.6674     18661\n",
      "           2     0.6674    0.3369    0.4478     19606\n",
      "           3     0.2903    0.0060    0.0118      1493\n",
      "\n",
      "    accuracy                         0.5733     39760\n",
      "   macro avg     0.5001    0.4034    0.3757     39760\n",
      "weighted avg     0.5946    0.5733    0.5345     39760\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:18<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[23803  2197     0]\n",
      " [10151  2099     0]\n",
      " [   53   284   793]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6999    0.9155    0.7933     26000\n",
      "           2     0.4583    0.1713    0.2494     12250\n",
      "           3     1.0000    0.7018    0.8248      1130\n",
      "\n",
      "    accuracy                         0.6779     39380\n",
      "   macro avg     0.7194    0.5962    0.6225     39380\n",
      "weighted avg     0.6334    0.6779    0.6250     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=4,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bba8ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [02:00<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:18<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[14922  3730     9]\n",
      " [11411  8178    17]\n",
      " [  566   917    10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5547    0.7996    0.6550     18661\n",
      "           2     0.6377    0.4171    0.5043     19606\n",
      "           3     0.2778    0.0067    0.0131      1493\n",
      "\n",
      "    accuracy                         0.5812     39760\n",
      "   macro avg     0.4901    0.4078    0.3908     39760\n",
      "weighted avg     0.5852    0.5812    0.5566     39760\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:17<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[21721  4279     0]\n",
      " [ 8697  3553     0]\n",
      " [   42   295   793]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7131    0.8354    0.7694     26000\n",
      "           2     0.4372    0.2900    0.3487     12250\n",
      "           3     1.0000    0.7018    0.8248      1130\n",
      "\n",
      "    accuracy                         0.6619     39380\n",
      "   macro avg     0.7168    0.6091    0.6476     39380\n",
      "weighted avg     0.6355    0.6619    0.6401     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=3,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddeb86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [02:02<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:21<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[16096  2557     8]\n",
      " [12642  6946    18]\n",
      " [  627   860     6]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5481    0.8625    0.6703     18661\n",
      "           2     0.6703    0.3543    0.4635     19606\n",
      "           3     0.1875    0.0040    0.0079      1493\n",
      "\n",
      "    accuracy                         0.5797     39760\n",
      "   macro avg     0.4686    0.4069    0.3806     39760\n",
      "weighted avg     0.5948    0.5797    0.5435     39760\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:20<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[23764  2236     0]\n",
      " [10083  2167     0]\n",
      " [   46   291   793]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7011    0.9140    0.7935     26000\n",
      "           2     0.4617    0.1769    0.2558     12250\n",
      "           3     1.0000    0.7018    0.8248      1130\n",
      "\n",
      "    accuracy                         0.6786     39380\n",
      "   macro avg     0.7209    0.5976    0.6247     39380\n",
      "weighted avg     0.6352    0.6786    0.6272     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=6,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d622f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "Reusing cached training data\n",
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n",
      "Reusing cached validation data\n",
      "Confusion Matrix:\n",
      " [[15428  3225     8]\n",
      " [11801  7790    15]\n",
      " [  569   922     2]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5550    0.8268    0.6642     18661\n",
      "           2     0.6526    0.3973    0.4939     19606\n",
      "           3     0.0800    0.0013    0.0026      1493\n",
      "\n",
      "    accuracy                         0.5840     39760\n",
      "   macro avg     0.4292    0.4085    0.3869     39760\n",
      "weighted avg     0.5853    0.5840    0.5554     39760\n",
      "\n",
      "Testing...\n",
      "Reusing cached test data\n",
      "Confusion Matrix:\n",
      " [[22711  3289     0]\n",
      " [ 9216  3034     0]\n",
      " [   38   301   791]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7105    0.8735    0.7836     26000\n",
      "           2     0.4580    0.2477    0.3215     12250\n",
      "           3     1.0000    0.7000    0.8235      1130\n",
      "\n",
      "    accuracy                         0.6738     39380\n",
      "   macro avg     0.7228    0.6071    0.6429     39380\n",
      "weighted avg     0.6403    0.6738    0.6410     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=7,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8b87a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "Reusing cached training data\n",
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n",
      "Reusing cached validation data\n",
      "Confusion Matrix:\n",
      " [[16117  2536     8]\n",
      " [12566  7023    17]\n",
      " [  614   873     6]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5501    0.8637    0.6721     18661\n",
      "           2     0.6732    0.3582    0.4676     19606\n",
      "           3     0.1935    0.0040    0.0079      1493\n",
      "\n",
      "    accuracy                         0.5821     39760\n",
      "   macro avg     0.4723    0.4086    0.3825     39760\n",
      "weighted avg     0.5974    0.5821    0.5463     39760\n",
      "\n",
      "Testing...\n",
      "Reusing cached test data\n",
      "Confusion Matrix:\n",
      " [[23852  2148     0]\n",
      " [10043  2207     0]\n",
      " [   41   296   793]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7029    0.9174    0.7959     26000\n",
      "           2     0.4745    0.1802    0.2612     12250\n",
      "           3     1.0000    0.7018    0.8248      1130\n",
      "\n",
      "    accuracy                         0.6819     39380\n",
      "   macro avg     0.7258    0.5998    0.6273     39380\n",
      "weighted avg     0.6404    0.6819    0.6304     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=8,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7186d4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "Reusing cached training data\n",
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n",
      "Reusing cached validation data\n",
      "Confusion Matrix:\n",
      " [[15607  3046     8]\n",
      " [11927  7664    15]\n",
      " [  560   931     2]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5555    0.8363    0.6676     18661\n",
      "           2     0.6584    0.3909    0.4905     19606\n",
      "           3     0.0800    0.0013    0.0026      1493\n",
      "\n",
      "    accuracy                         0.5853     39760\n",
      "   macro avg     0.4313    0.4095    0.3869     39760\n",
      "weighted avg     0.5884    0.5853    0.5553     39760\n",
      "\n",
      "Testing...\n",
      "Reusing cached test data\n",
      "Confusion Matrix:\n",
      " [[23040  2960     0]\n",
      " [ 9363  2887     0]\n",
      " [   35   304   791]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7103    0.8862    0.7885     26000\n",
      "           2     0.4694    0.2357    0.3138     12250\n",
      "           3     1.0000    0.7000    0.8235      1130\n",
      "\n",
      "    accuracy                         0.6785     39380\n",
      "   macro avg     0.7265    0.6073    0.6419     39380\n",
      "weighted avg     0.6436    0.6785    0.6419     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=9,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "822181f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "Reusing cached training data\n",
      "Model saved to finished_models/baselines/knn_model.pkl\n",
      "Validation...\n",
      "Reusing cached validation data\n",
      "Confusion Matrix:\n",
      " [[16136  2516     9]\n",
      " [12512  7078    16]\n",
      " [  602   889     2]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5517    0.8647    0.6736     18661\n",
      "           2     0.6752    0.3610    0.4705     19606\n",
      "           3     0.0741    0.0013    0.0026      1493\n",
      "\n",
      "    accuracy                         0.5839     39760\n",
      "   macro avg     0.4336    0.4090    0.3822     39760\n",
      "weighted avg     0.5946    0.5839    0.5482     39760\n",
      "\n",
      "Testing...\n",
      "Reusing cached test data\n",
      "Confusion Matrix:\n",
      " [[23907  2093     0]\n",
      " [10057  2193     0]\n",
      " [   41   297   792]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7030    0.9195    0.7968     26000\n",
      "           2     0.4785    0.1790    0.2606     12250\n",
      "           3     1.0000    0.7009    0.8241      1130\n",
      "\n",
      "    accuracy                         0.6829     39380\n",
      "   macro avg     0.7272    0.5998    0.6272     39380\n",
      "weighted avg     0.6417    0.6829    0.6308     39380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions, Y_val, test_predictions, Y_test, knn = train_knn_classifier(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    n_neighbors=10,\n",
    "    load_model=False,\n",
    "    model_filename=model_filename\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332209e",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56601b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n",
    "def evaluate_performance(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the model by calculating accuracy, confusion matrix, and classification report.\n",
    "\n",
    "    Parameters:\n",
    "        Y_true (np.ndarray): True labels.\n",
    "        Y_pred (np.ndarray): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics (accuracy, confusion matrix, class-wise accuracy, and classification report).\n",
    "    \"\"\"\n",
    "    # Calculate balanced accuracy score\n",
    "    total_accuracy = balanced_accuracy_score(Y_true, Y_pred)\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(Y_true, Y_pred)\n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "    # Get the classification report\n",
    "    class_report = classification_report(Y_true, Y_pred, digits=4)\n",
    "    # Return all metrics in a dictionary\n",
    "    return {\n",
    "        'total_accuracy': total_accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "    \n",
    "def mapping_JV_prop(clusters, true_labels):\n",
    "    \"\"\"\n",
    "    Map clusters to true labels using the modified Jonker-Volgenant algorithm, \n",
    "    considering the proportional presence of each class in each cluster.\n",
    "    This approach dynamically handles the many-to-one relationship \n",
    "    between clusters and true labels.\n",
    "\n",
    "    Args:\n",
    "    - clusters: Array of cluster assignments for each sample.\n",
    "    - true_labels: Array of true labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where keys are clusters, and values are true labels (each cluster maps to one class).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique clusters and labels\n",
    "    all_clusters = np.unique(clusters)\n",
    "    all_labels = np.unique(true_labels)\n",
    "    \n",
    "    # Compute global counts for each label (assuming labels start at 1)\n",
    "    global_counts = dict(enumerate(np.bincount(true_labels)[1:], start=1))\n",
    "    \n",
    "    k = len(all_clusters)\n",
    "    c = len(all_labels)\n",
    "    max_clusters_per_class = k - c + 1\n",
    "    # print(max_clusters_per_class)\n",
    "    \n",
    "    # Build the initial cost matrix: rows for clusters, columns for labels\n",
    "    cost_matrix = np.zeros((len(all_clusters), len(all_labels)*max_clusters_per_class), dtype=float)\n",
    "    \n",
    "    for i, cluster in enumerate(all_clusters):\n",
    "        # Get indices for samples in this cluster\n",
    "        indices = np.where(clusters == cluster)[0]\n",
    "        \n",
    "        for j, label in enumerate(all_labels):\n",
    "            # Count the occurrences of this label in the cluster\n",
    "            count = np.sum(true_labels[indices] == label)\n",
    "            \n",
    "            # Calculate proportion of this label in the cluster\n",
    "            proportion = count / global_counts[label] if global_counts[label] else 0\n",
    "            cost_matrix[i, j] = proportion\n",
    "            # duplicate the columns to allow for multiple clusters to 1 class\n",
    "            for dupe_idx in range(1, max_clusters_per_class):\n",
    "                # print(j, dupe_idx, j + len(all_labels)*dupe_idx)\n",
    "                cost_matrix[i, j + (len(all_labels)*dupe_idx)] = proportion  # Duplicate each label\n",
    "    \n",
    "    # print(cost_matrix.shape)\n",
    "    # Apply the modified Jonker-Volgenant algorithm (linear sum assignment)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix, maximize=True)\n",
    "    \n",
    "    # print(row_ind, col_ind)\n",
    "    \n",
    "    # Initialize a dictionary for mapping clusters to labels\n",
    "    # cluster_to_label_mapping = {cluster: [] for cluster in all_clusters}\n",
    "    cluster_to_label_mapping = {}\n",
    "    \n",
    "    # Perform the dynamic assignment\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        cluster = all_clusters[r]\n",
    "        label = all_labels[c % len(all_labels)]\n",
    "        cluster_to_label_mapping[cluster] = label\n",
    "    \n",
    "    # Now `cluster_to_label_mapping` contains the many-to-one mapping of clusters to classes\n",
    "    return cluster_to_label_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d2a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:20<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model with k=3 clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [09:24<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to kmeans_baseline_model_k3_29_04_2025_14_18.pkl\n",
      "Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:22<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 1, 2: 3}\n",
      "Confusion Matrix: [[12928  4982   751]\n",
      " [ 7123  9237  3246]\n",
      " [  175   420   898]]\n",
      "Per-class accuracy: [0.69278174 0.47113129 0.60147354]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6392    0.6928    0.6649     18661\n",
      "           2     0.6310    0.4711    0.5395     19606\n",
      "           3     0.1835    0.6015    0.2812      1493\n",
      "\n",
      "    accuracy                         0.5801     39760\n",
      "   macro avg     0.4845    0.5885    0.4952     39760\n",
      "weighted avg     0.6180    0.5801    0.5886     39760\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:20<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[21549  3877   574]\n",
      " [ 8199  3289   762]\n",
      " [  800   133   197]]\n",
      "Per-class accuracy: [0.82880769 0.2684898  0.17433628]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7054    0.8288    0.7621     26000\n",
      "           2     0.4506    0.2685    0.3365     12250\n",
      "           3     0.1285    0.1743    0.1480      1130\n",
      "\n",
      "    accuracy                         0.6357     39380\n",
      "   macro avg     0.4282    0.4239    0.4155     39380\n",
      "weighted avg     0.6096    0.6357    0.6121     39380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# new kmeans train with k selection\n",
    "from sklearn.metrics import silhouette_score\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import os\n",
    "\n",
    "def plot_scores(scores):\n",
    "    \"\"\"\n",
    "    Plot the provided scores (either WSS or Silhouette Scores) to help determine the optimal number of clusters.\n",
    "    \n",
    "    Args:\n",
    "        scores (list or array): The list or array of score values to plot (WSS or Silhouette).\n",
    "    \"\"\"\n",
    "    # Plot the scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(3, 10+1), scores, marker='o')\n",
    "    plt.title('Score vs. Number of Clusters')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the optimal k based on the highest score\n",
    "    # optimal_k = np.argmax(scores) + 2  # +2 because range starts at 2\n",
    "    # print(f\"Optimal k based on Score: {optimal_k}\")\n",
    "\n",
    "def select_best_k(train_dataset, val_dataset, batch_size, min_k, max_k, random_state=42):\n",
    "    \"\"\"\n",
    "    Determines the optimal number of clusters using the silhouette score.\n",
    "\n",
    "    Parameters:\n",
    "        train_dataset (Dataset): Training dataset (no annotations needed).\n",
    "        val_dataset (Dataset): Validation dataset (with annotations).\n",
    "        batch_size (int): Batch size for training.\n",
    "        min_k (int): Minimum number of clusters to test.\n",
    "        max_k (int): Maximum number of clusters to test.\n",
    "        random_state (int): Random seed.\n",
    "\n",
    "    Returns:\n",
    "        int: The best number of clusters based on silhouette score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare validation data\n",
    "    X_val, _ = prepare_ann_data(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))\n",
    "    \n",
    "    best_k = min_k\n",
    "    best_score = -1\n",
    "    \n",
    "    wss = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "\n",
    "    for k in range(min_k, max_k + 1):\n",
    "        print(f\"Evaluating KMeans with k={k} clusters...\")\n",
    "        \n",
    "        # Initialize MiniBatchKMeans\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, random_state=random_state)\n",
    "        \n",
    "        # Train the model on the training dataset\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for i, (signals, batch_labels) in enumerate(tqdm.tqdm(train_loader)):\n",
    "            batch_features = batched_feature_extraction(signals)\n",
    "            kmeans.partial_fit(batch_features)\n",
    "\n",
    "        # Predict validation clusters\n",
    "        val_predictions = kmeans.predict(X_val)\n",
    "\n",
    "        wss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Compute silhouette score (higher is better)\n",
    "        score = silhouette_score(X_val, val_predictions)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Silhouette Score for k={k}: {score}\")\n",
    "\n",
    "        # Update best k if this score is higher\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    print(f\"Best k found: {best_k} with silhouette score: {best_score}\")\n",
    "    print('sil ', silhouette_scores)\n",
    "    print('eblow ', wss)\n",
    "    return best_k, silhouette_scores, wss\n",
    "\n",
    "\n",
    "def train_kmeans(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    model_filename=None,\n",
    "    min_k=None,\n",
    "    max_k=10,\n",
    "    batch_size=32,\n",
    "    random_state=42,\n",
    "    load_model=False,\n",
    "    num_clusters=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train or load a MiniBatchKMeans model, selecting the best k using silhouette score.\n",
    "\n",
    "    Parameters:\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        val_dataset (Dataset): Validation dataset.\n",
    "        test_dataset (Dataset): Test dataset.\n",
    "        model_filename (str): Path to save or load the model.\n",
    "        min_k (int): Minimum number of clusters (defaults to num_classes).\n",
    "        max_k (int): Maximum number of clusters.\n",
    "        batch_size (int): Batch size for training.\n",
    "        random_state (int): Random seed.\n",
    "        load_model (bool): Whether to load the model instead of training.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels for the validation dataset.\n",
    "    \"\"\"\n",
    "    # Determine number of classes (min_k)\n",
    "    print('Preparing features...')\n",
    "    _, Y_val = prepare_ann_data(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))\n",
    "    num_classes = len(np.unique(Y_val))\n",
    "    min_k = min_k if min_k else num_classes\n",
    "    if load_model and os.path.exists(model_filename):\n",
    "        print(f\"Loading model from {model_filename}\")\n",
    "        kmeans = joblib.load(model_filename)\n",
    "    else:\n",
    "        if not num_clusters:\n",
    "            print(\"Selecting best number of clusters...\")\n",
    "            best_k, sil, wss = select_best_k(train_dataset, val_dataset, batch_size, min_k, max_k, random_state)\n",
    "            plot_scores(sil)\n",
    "            plot_scores(wss)\n",
    "        else:\n",
    "            best_k = num_clusters\n",
    "                \n",
    "        print(f\"Training final model with k={best_k} clusters...\")\n",
    "        kmeans = MiniBatchKMeans(n_clusters=best_k, batch_size=batch_size, random_state=random_state)\n",
    "        \n",
    "        # train_dataset[np.random.choice(range(len(train_dataset)), 96*100)]\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        for i, (signals, batch_labels) in enumerate(tqdm.tqdm(train_loader)):\n",
    "            batch_features = batched_feature_extraction(signals)\n",
    "            kmeans.partial_fit(batch_features)\n",
    "            \n",
    "        current_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M\")\n",
    "        model_filename = f\"kmeans_baseline_model_k{best_k}_{current_time}.pkl\"\n",
    "        joblib.dump(kmeans, model_filename)\n",
    "        print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    # Validation phase\n",
    "    print('Validation...')\n",
    "    X_val, Y_val = prepare_ann_data(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))\n",
    "    val_predictions = kmeans.predict(X_val)\n",
    "    \n",
    "    cluster_to_label = mapping_JV_prop(val_predictions, Y_val)\n",
    "    print(cluster_to_label)\n",
    "    val_labels_pred = np.array([cluster_to_label[cluster] for cluster in val_predictions])\n",
    "\n",
    "    val_metrics = evaluate_performance(Y_val, val_labels_pred)\n",
    "    print(\"Confusion Matrix:\", val_metrics['confusion_matrix'])\n",
    "    print(\"Per-class accuracy:\", val_metrics['class_accuracy'])\n",
    "    print(val_metrics['classification_report'])\n",
    "\n",
    "    # Testing phase\n",
    "    print('Testing...')\n",
    "    X_test, Y_test = prepare_ann_data(DataLoader(test_dataset, batch_size=batch_size, shuffle=False))\n",
    "    test_predictions = kmeans.predict(X_test)\n",
    "\n",
    "    test_labels_pred = np.array([cluster_to_label[cluster] for cluster in test_predictions])\n",
    "    test_metrics = evaluate_performance(Y_test, test_labels_pred)\n",
    "    \n",
    "    print(\"Confusion Matrix:\", test_metrics['confusion_matrix'])\n",
    "    print(\"Per-class accuracy:\", test_metrics['class_accuracy'])\n",
    "    print(test_metrics['classification_report'])\n",
    "\n",
    "    return val_labels_pred\n",
    "\n",
    "\n",
    "# model storage name\n",
    "window_size = 2.5  # in seconds\n",
    "sampling_frequency = 100  # Hz\n",
    "stride = None\n",
    "\n",
    "window_size = window_size * sampling_frequency\n",
    "\n",
    "# Use TrainDataset for training data (no annotations)\n",
    "mode = None # not 'random' since kmeans is not optimization method\n",
    "train_dataset = TrainDataset(data_path, train_records, window_size, sampling_frequency, stride, mode=mode)\n",
    "# Use AnnotatedDataset for validation data (with annotations)\n",
    "val_dataset = AnnotatedDataset(data_path, validation_records, window_size, sampling_frequency, stride, balanced_classes=False)\n",
    "\n",
    "test_dataset = AnnotatedDataset(data_path, test_records, window_size, sampling_frequency, stride, balanced_classes=False)\n",
    "\n",
    "predictions = train_kmeans(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    load_model=True,\n",
    "    model_filename=\"finished_models/baselines//kmeans_model.pkl\",\n",
    "    num_clusters=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4c212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
